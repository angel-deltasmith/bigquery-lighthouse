{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM8mgI7xQv0oihyKkO22w2Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angel-deltasmith/bigquery-lighthouse/blob/main/LIghthouse_working_example\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fDqs8tvo0P5"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade google-cloud-BigQuery\n",
        "!export GOOGLE_APPLICATION_CREDENTIALS=\"/content/dbtkey.json\"\n",
        "!pip3 install --upgrade google-cloud-bigquery\n",
        "!pip install --upgrade google-cloud\n",
        "!pip install --upgrade google-cloud-bigquery\n",
        "!pip install --upgrade google-cloud-bigquery\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud.bigquery.client import Client\n",
        "from google.cloud import bigquery\n",
        "client = Client.from_service_account_json('/content/dbtkey.json', project='dbt-project-335000')\n",
        "\n",
        "table_id_output = 'dbt-project-335000.dbt_mangel.output2'\n",
        "job_config = bigquery.QueryJobConfig(destination=table_id_output)#parameters = destination table to write \n",
        "job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND #If the table already exists, then append the rows\n",
        "\n",
        "#Get the values from the input table\n",
        "sql = \"\"\"\n",
        "    SELECT \n",
        "    * \n",
        "  FROM `dbt-project-335000.dbt_mangel.input_test`;\n",
        "\"\"\"\n",
        "\n",
        "# Start the query, passing in the extra configuration.\n",
        "query_job = client.query(sql, job_config=job_config)  # Make an API request.\n",
        "query_job.result()  # Wait for the job to complete.\n",
        "\n",
        "\n",
        "print(\"Query results loaded to the table {}\".format(table_id_output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZM6mTh6pJ63",
        "outputId": "954072d0-ea49-4771-f789-d050f9a0155c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query results loaded to the table dbt-project-335000.dbt_mangel.output2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import time\n",
        "#from numpy.lib.function_base import extract\n",
        "from datetime import datetime\n",
        "from os.path import join\n",
        "from openpyxl import Workbook\n",
        "wb1 = Workbook()\n",
        "wb2 = Workbook()\n",
        "\n",
        "### create needed variables\n",
        "\n",
        "name = \"Report\"\n",
        "getdate = datetime.now().strftime(\"%m-%d-%y\")\n",
        "\n",
        "### set relative_path MAC/WINDOWS\n",
        "\n",
        "relative_path = '/content/'  ### WINDOWS -> \\\\..\\\\..\\\\\n",
        "#relative_path = 'C:\\\\Users\\\\mlope\\\\OneDrive\\\\Documentos\\\\Lighthouse\\\\'  ### WINDOWS -> \\\\..\\\\..\\\\\n",
        "\n",
        "csv_file_mob = join(relative_path, 'lighthouse_mobile_' + getdate + '.csv')\n",
        "csv_file_des = join(relative_path, 'lighthouse_desktop_' + getdate + '.csv')\n",
        "\n",
        "ws_mob = wb1.active;\n",
        "ws_des = wb2.active;\n",
        "\n",
        "\n",
        "def last_row_mob(): return len(ws_mob['A'])\n",
        "def last_row_des(): return len(ws_des['A'])\n",
        "\n",
        "### initialize an array with the links you want to run the Lighthouse script on\n",
        "\n",
        "urls = [\n",
        "\"https://arctictravelcompany.com/\",\n",
        "\"http://www.fjellheisen.no\"\n",
        "]\n",
        "\n",
        "### set the 'base' object, in Python AKA - dictionary; for setting the Header forEach iteration in Excel based on the num_of_call\n",
        "### if the value is getting higher than 6 -> eg: num_of_calls = 7, then you need to create another key-value -> 7: 'Seventh Run',\n",
        "base = {\n",
        "    1: 'First Run',\n",
        "    2: 'Second Run',\n",
        "    3: 'Third Run',\n",
        "    4: 'Fourth Run',\n",
        "    5: 'Fifth Run',\n",
        "    6: 'Sixth Run',\n",
        "}\n",
        "\n",
        "def extract_info(run, preset):\n",
        "\n",
        "    header = [run, 'Product_Name', 'URL', 'First_Contentful_Paint', 'Speed_Index', 'Largest_Contentful_Paint', 'Performance']\n",
        "    if preset == 'desktop':     ### preset -> 2 values: 'desktop' & 'perf'(for mobile)\n",
        "        last = last_row_des()+1\n",
        "        working = ws_des\n",
        "    else:\n",
        "        last = last_row_mob()+1\n",
        "        working = ws_mob\n",
        "\n",
        "    if 'first' not in run.lower():\n",
        "        last += 1\n",
        "\n",
        "    for i, r in enumerate('ABCDEFG'):\n",
        "\n",
        "        working[r+str(last)].value = header[i]\n",
        "\n",
        "    for url in urls:\n",
        "        print('INFO: Lighthouse call Opened!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
        "        stream = os.popen('lighthouse --disable-storage-reset=true --preset=' +\n",
        "                          preset + ' --output=json --output-path='+relative_path + name+'_'+getdate+'.report.json ' + url)\n",
        "        print('INFO: Lighthouse call finished!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
        "        #print('lighthouse --disable-storage-reset=true --preset=' +preset + ' --output=json --output-path='+relative_path + name+'_'+getdate+'.report.json ' + url)\n",
        "        time.sleep(60)\n",
        "        json_filename = join(relative_path+name+ '_' +getdate + '.report.json ')\n",
        "        #print(relative_path+name+ '_' +getdate + '.report.json')\n",
        "        ### open the JSON FILE and start processing it\n",
        "\n",
        "        print('INFO: Creating JSON FILE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
        "        with open(json_filename, encoding=\"utf8\") as json_data:\n",
        "            loaded_json = json.load(json_data)\n",
        "            \n",
        "                          \n",
        "        print('INFO: JSON Created!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
        "\n",
        "        ### set the items you need from the JSON FILE here\n",
        "        try: \n",
        "            Product_Name = loaded_json[\"audits\"][\"largest-contentful-paint-element\"][\"details\"][\"items\"][0][\"node\"][\"nodeLabel\"] ### get the name of the product to be descriptive\n",
        "            First_Contentful_Paint = str(round(loaded_json[\"audits\"][\"first-contentful-paint\"][\"score\"] * 100))\n",
        "            Speed_Index = str(round(loaded_json[\"audits\"][\"speed-index\"][\"score\"] * 100))\n",
        "            Largest_Contentful_Paint = str(round(loaded_json[\"audits\"][\"largest-contentful-paint\"][\"score\"] * 100))\n",
        "            Performance = str(round(loaded_json[\"categories\"][\"performance\"][\"score\"] * 100))\n",
        "        except Exception as e:\n",
        "            Product_Name = First_Contentful_Paint = Speed_Index = Largest_Contentful_Paint = Performance = '-'\n",
        "            print(e)\n",
        "\n",
        "        ### (1) if you want to add a new column for the report you need to create a new var as fcps, fcpdv for eg. (see below) -- these are coming from JSON FILE\n",
        "        ### (2) go to the line, where the 'header' AKA dataFrame is initialised (above all - even under the declaration of the function) and add the new item as a string for eg: [..., 'lcpdv']\n",
        "        ### (3) go to the line where the enumeration was set and add another letter which corresponds with the next column in excel [...,JKLMN...]\n",
        "        ### (4) go to the line where we set the 'header' for the excel report -> data: [urls.index(url),...., lcpdv]\n",
        "        ### (5) go to the line where the enumeration was set and add another letter which corresponds with the next column in excel [...,JKLMN...]\n",
        "\n",
        "        ### if you increase the columns for your report, don't forget to add them below -> data = [..., sidv] !!! steps -> (4) - (5)\n",
        "        data = [urls.index(url), Product_Name, url, First_Contentful_Paint, Speed_Index, Largest_Contentful_Paint, Performance]\n",
        "        if preset == 'desktop':\n",
        "            last = last_row_des()+1\n",
        "        else:\n",
        "            last = last_row_mob()+1\n",
        "        for i, r in enumerate('ABCDEFG'):\n",
        "            working[r+str(last)].value = data[i]\n",
        "\n",
        "### here you can set how many times to run the test on the links\n",
        "\n",
        "num_of_call = 1\n",
        "for i in range(1, num_of_call+1):\n",
        "    #extract_info(base[i], preset='perf') ### run the test on mobile\n",
        "    extract_info(base[i], preset='desktop') ### run the test on desktop\n",
        "\n",
        "wb1.save(csv_file_mob)  ### save the results for mobile in an EXCEL FILE\n",
        "wb2.save(csv_file_des) ### save the results for desktop in an EXCEL FILE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "yyQJ9y__qb7V",
        "outputId": "2323ddf7-e1fe-497d-a0bd-72e8dcb37e0a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Lighthouse call Opened!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "INFO: Lighthouse call finished!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "INFO: Creating JSON FILE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c8c5ad5c2c7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_call\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;31m#extract_info(base[i], preset='perf') ### run the test on mobile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mextract_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'desktop'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m### run the test on desktop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0mwb1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file_mob\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m### save the results for mobile in an EXCEL FILE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-c8c5ad5c2c7b>\u001b[0m in \u001b[0;36mextract_info\u001b[0;34m(run, preset)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'INFO: Creating JSON FILE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0mloaded_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Report_12-28-21.report.json '"
          ]
        }
      ]
    }
  ]
}